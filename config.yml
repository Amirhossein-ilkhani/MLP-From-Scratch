batch: 32
epochs: 10
input: 'Standardaized' # Normalized/Standardaized/Nothing
learning_rate: 0.001
input_layer: [3072]
hidden_layer: [16]
output_layer: [10]
momentum: 'True'  # True/False
af: 'Relu' # Relu / sigmoid / Tanh / Leaky_Relu / nothing
af_last_layer: 'softmax' # softmax / sigmoid / Relu / nothing / Tanh
loss: 'Cross_Entropy' # Cross_Entropy / MSE
mu: 0
sigma: 1
bias: 0





  